{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea4eb61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import json\n",
    "\n",
    "\n",
    "def extract_tables_to_json(pdf_path, output_path):\n",
    "    tables_data = []\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages):\n",
    "            # 提取当前页面的所有表格\n",
    "            tables = page.extract_tables()\n",
    "\n",
    "            for table_index, table in enumerate(tables):\n",
    "                # 将表格转换为字典格式\n",
    "                table_dict = {\n",
    "                    'page': page_num + 1,\n",
    "                    'table_index': table_index,\n",
    "                    'data': table,\n",
    "                }\n",
    "                tables_data.append(table_dict)\n",
    "\n",
    "    # 保存为JSON文件\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(tables_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "extract_tables_to_json(r'D:\\user\\PythonProject\\AI_env2\\uploads\\1_tender_招标文件正文.pdf', 'tables.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94962e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6d2d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import json\n",
    "\n",
    "\n",
    "def extract_tables_with_headers(pdf_path, output_path):\n",
    "    tables_data = []\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages):\n",
    "            tables = page.extract_tables()\n",
    "\n",
    "            for table_index, table in enumerate(tables):\n",
    "                if not table:\n",
    "                    continue\n",
    "\n",
    "                # 假设第一行为表头\n",
    "                headers = table[0] if table else []\n",
    "                rows = table[1:] if len(table) > 1 else []\n",
    "\n",
    "                # 转换为键值对格式\n",
    "                table_rows = []\n",
    "                for row in rows:\n",
    "                    if len(row) == len(headers):\n",
    "                        row_dict = dict(zip(headers, row))\n",
    "                        table_rows.append(row_dict)\n",
    "\n",
    "                table_dict = {\n",
    "                    'page': page_num + 1,\n",
    "                    'table_index': table_index,\n",
    "                    'headers': headers,\n",
    "                    'rows': table_rows,\n",
    "                }\n",
    "                tables_data.append(table_dict)\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(tables_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "extract_tables_with_headers(r'D:\\user\\PythonProject\\AI_env2\\uploads\\1_tender_招标文件正文.pdf', 'tables_with_headers.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90f6a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import json\n",
    "\n",
    "\n",
    "def extract_and_smart_merge_tables(pdf_path, output_path, table_groups):\n",
    "    \"\"\"\n",
    "    智能合并表格（基于表格组定义）\n",
    "    :param pdf_path: PDF文件路径\n",
    "    :param output_path: 输出JSON文件路径\n",
    "    :param table_groups: 表格组定义，如 [{\"pages\": [4, 5, 6, 7], \"name\": \"招标要求\"}, ...]\n",
    "    \"\"\"\n",
    "    tables_data = []\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for group_index, group in enumerate(table_groups):\n",
    "            merged_table = []\n",
    "            first_headers = None\n",
    "\n",
    "            # 遍历组内所有页面\n",
    "            for page_num in group['pages']:\n",
    "                if page_num <= len(pdf.pages):\n",
    "                    page = pdf.pages[page_num - 1]\n",
    "                    tables = page.extract_tables()\n",
    "\n",
    "                    # 处理当前页面的所有表格\n",
    "                    for table in tables:\n",
    "                        if table:\n",
    "                            # 第一个表格的表头作为标准\n",
    "                            if first_headers is None:\n",
    "                                first_headers = table[0] if table else []\n",
    "                                merged_table.extend(table)\n",
    "                            else:\n",
    "                                # 检查是否与第一个表格结构匹配\n",
    "                                if len(table[0]) == len(first_headers):\n",
    "                                    # 去掉表头行再合并\n",
    "                                    data_rows = table[1:] if len(table) > 1 else []\n",
    "                                    merged_table.extend(data_rows)\n",
    "                                else:\n",
    "                                    # 结构不匹配，当作独立表格处理\n",
    "                                    table_dict = {\n",
    "                                        'page': page_num,\n",
    "                                        'group': group.get(\n",
    "                                            'name', f'Group {group_index}'\n",
    "                                        ),\n",
    "                                        'headers': table[0] if table else [],\n",
    "                                        'rows': [\n",
    "                                            dict(zip(table[0] if table else [], row))\n",
    "                                            for row in table[1:]\n",
    "                                            if table\n",
    "                                            and len(row)\n",
    "                                            == len(table[0] if table else [])\n",
    "                                        ],\n",
    "                                    }\n",
    "                                    tables_data.append(table_dict)\n",
    "\n",
    "            # 处理合并后的表格\n",
    "            if merged_table and first_headers:\n",
    "                rows = merged_table[1:] if len(merged_table) > 1 else []\n",
    "\n",
    "                # 转换为键值对格式\n",
    "                table_rows = []\n",
    "                for row in rows:\n",
    "                    if len(row) == len(first_headers):\n",
    "                        row_dict = dict(zip(first_headers, row))\n",
    "                        table_rows.append(row_dict)\n",
    "\n",
    "                table_dict = {\n",
    "                    'group': group.get('name', f'Group {group_index}'),\n",
    "                    'pages': group['pages'],\n",
    "                    'headers': first_headers,\n",
    "                    'rows': table_rows,\n",
    "                }\n",
    "                tables_data.append(table_dict)\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(tables_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "# table_groups = [\n",
    "#     {'pages': [4, 5, 6, 7, 8], 'name': '招标基本信息'},\n",
    "#     {'pages': [14, 15, 16], 'name': '评标标准'},\n",
    "#     {'pages': [27, 28], 'name': '设备清单'},\n",
    "#     {'pages': [45, 46], 'name': '责任分工'},\n",
    "# ]\n",
    "table_groups = [    \n",
    "    {'pages': [14, 15, 16], 'name': '评标标准'}\n",
    "]\n",
    "\n",
    "extract_and_smart_merge_tables(\n",
    "    r'D:\\user\\PythonProject\\AI_env2\\uploads\\1_tender_招标文件正文.pdf',\n",
    "    'grouped_merged_tables.json',\n",
    "    table_groups,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31e5aed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import json\n",
    "\n",
    "\n",
    "def comprehensive_table_extraction(pdf_path, output_path, config):\n",
    "    \"\"\"\n",
    "    综合表格提取方案\n",
    "    :param pdf_path: PDF文件路径\n",
    "    :param output_path: 输出JSON文件路径\n",
    "    :param config: 配置信息，包含各种表格处理规则\n",
    "    \"\"\"\n",
    "    tables_data = []\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        # 1. 处理独立页面表格\n",
    "        if 'single_pages' in config:\n",
    "            for page_num in config['single_pages']:\n",
    "                if page_num <= len(pdf.pages):\n",
    "                    page = pdf.pages[page_num - 1]\n",
    "                    tables = page.extract_tables()\n",
    "\n",
    "                    for table_index, table in enumerate(tables):\n",
    "                        if table:\n",
    "                            headers = table[0] if table else []\n",
    "                            rows = table[1:] if len(table) > 1 else []\n",
    "\n",
    "                            table_rows = []\n",
    "                            for row in rows:\n",
    "                                if len(row) == len(headers):\n",
    "                                    row_dict = dict(zip(headers, row))\n",
    "                                    table_rows.append(row_dict)\n",
    "\n",
    "                            table_dict = {\n",
    "                                'type': 'single_page',\n",
    "                                'page': page_num,\n",
    "                                'table_index': table_index,\n",
    "                                'headers': headers,\n",
    "                                'rows': table_rows,\n",
    "                            }\n",
    "                            tables_data.append(table_dict)\n",
    "\n",
    "        # 2. 处理合并表格\n",
    "        if 'merged_ranges' in config:\n",
    "            for range_info in config['merged_ranges']:\n",
    "                merged_table = []\n",
    "                first_headers = None\n",
    "                pages = range_info['pages']\n",
    "\n",
    "                for page_num in pages:\n",
    "                    if page_num <= len(pdf.pages):\n",
    "                        page = pdf.pages[page_num - 1]\n",
    "                        tables = page.extract_tables()\n",
    "\n",
    "                        if tables and tables[0]:\n",
    "                            table = tables[0]\n",
    "                            if first_headers is None:\n",
    "                                first_headers = table[0] if table else []\n",
    "                                merged_table.extend(table)\n",
    "                            else:\n",
    "                                data_rows = table[1:] if len(table) > 1 else []\n",
    "                                merged_table.extend(data_rows)\n",
    "\n",
    "                if merged_table and first_headers:\n",
    "                    rows = merged_table[1:] if len(merged_table) > 1 else []\n",
    "\n",
    "                    table_rows = []\n",
    "                    for row in rows:\n",
    "                        if len(row) == len(first_headers):\n",
    "                            row_dict = dict(zip(first_headers, row))\n",
    "                            table_rows.append(row_dict)\n",
    "\n",
    "                    table_dict = {\n",
    "                        'type': 'merged',\n",
    "                        'name': range_info.get('name', ''),\n",
    "                        'pages': pages,\n",
    "                        'headers': first_headers,\n",
    "                        'rows': table_rows,\n",
    "                    }\n",
    "                    tables_data.append(table_dict)\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(tables_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "config = {\n",
    "    # 'single_pages': [51, 72, 73, 79, 80, 82, 83],  # 独立表格页面\n",
    "    'merged_ranges': [\n",
    "        # {'name': '招标要求', 'pages': [4, 5, 6, 7, 8]},\n",
    "        {'name': '评标标准', 'pages': [14, 15, 16]},\n",
    "        # {'name': '设备清单', 'pages': [27, 28]},\n",
    "        # {'name': '责任分工', 'pages': [45, 46]},\n",
    "    ],\n",
    "}\n",
    "\n",
    "comprehensive_table_extraction(r'D:\\user\\PythonProject\\AI_env2\\uploads\\1_tender_招标文件正文.pdf', 'comprehensive_tables.json', config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0205182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'page': 5, 'method': 'chars', 'text': '评标办法', 'bbox': {'x0': 256.67865, 'y0': 525.3029999999999, 'x1': 298.68729999999994, 'y1': 535.7529999999999}, 'center': {'x': 277.68297499999994, 'y': 530.5279999999999}, 'page_size': {'width': 595.25, 'height': 841.9}}, {'page': 9, 'method': 'chars', 'text': '评标办法', 'bbox': {'x0': 115.55985000000001, 'y0': 446.34299999999996, 'x1': 157.5685, 'y1': 456.793}, 'center': {'x': 136.564175, 'y': 451.568}, 'page_size': {'width': 595.25, 'height': 841.9}}, {'page': 13, 'method': 'chars', 'text': '评标办法', 'bbox': {'x0': 202.08, 'y0': 282.903, 'x1': 244.80970000000002, 'y1': 293.35299999999995}, 'center': {'x': 223.44485000000003, 'y': 288.128}, 'page_size': {'width': 595.25, 'height': 841.9}}, {'page': 13, 'method': 'chars', 'text': '评标办法', 'bbox': {'x0': 96.0, 'y0': 303.3030000000001, 'x1': 138.6094, 'y1': 313.75300000000004}, 'center': {'x': 117.3047, 'y': 308.5280000000001}, 'page_size': {'width': 595.25, 'height': 841.9}}, {'page': 14, 'method': 'chars', 'text': '评标办法', 'bbox': {'x0': 70.8, 'y0': 323.703, 'x1': 538.4495499999999, 'y1': 354.672}, 'center': {'x': 304.62477499999994, 'y': 339.1875}, 'page_size': {'width': 595.25, 'height': 841.9}}, {'page': 14, 'method': 'chars', 'text': '评标办法', 'bbox': {'x0': 304.68, 'y0': 491.0432, 'x1': 392.62850000000003, 'y1': 512.9932}, 'center': {'x': 348.65425000000005, 'y': 502.0182}, 'page_size': {'width': 595.25, 'height': 841.9}}]\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import json\n",
    "\n",
    "\n",
    "class PDFTextLocator:\n",
    "    def __init__(self, pdf_path):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.pdf = pdfplumber.open(pdf_path)\n",
    "\n",
    "    def find_text(self, search_text, method='chars', case_sensitive=True):\n",
    "        \"\"\"\n",
    "        查找文本位置的主方法\n",
    "        :param search_text: 要查找的文本\n",
    "        :param method: 查找方法 ('chars', 'words', 'text')\n",
    "        :param case_sensitive: 是否区分大小写\n",
    "        \"\"\"\n",
    "        positions = []\n",
    "\n",
    "        for page_num, page in enumerate(self.pdf.pages, 1):\n",
    "            if method == 'chars':\n",
    "                positions.extend(\n",
    "                    self._find_with_chars(page, page_num, search_text, case_sensitive)\n",
    "                )\n",
    "            elif method == 'words':\n",
    "                positions.extend(\n",
    "                    self._find_with_words(page, page_num, search_text, case_sensitive)\n",
    "                )\n",
    "            elif method == 'text':\n",
    "                positions.extend(\n",
    "                    self._find_with_text(page, page_num, search_text, case_sensitive)\n",
    "                )\n",
    "\n",
    "        return positions\n",
    "\n",
    "    def _find_with_chars(self, page, page_num, search_text, case_sensitive):\n",
    "        \"\"\"使用字符对象查找\"\"\"\n",
    "        positions = []\n",
    "        chars = page.chars\n",
    "        search = search_text if case_sensitive else search_text.lower()\n",
    "\n",
    "        for i in range(len(chars) - len(search_text) + 1):\n",
    "            match = True\n",
    "            matched_chars = []\n",
    "\n",
    "            for j in range(len(search_text)):\n",
    "                if i + j < len(chars):\n",
    "                    char_text = chars[i + j].get('text', '')\n",
    "                    compare_text = char_text if case_sensitive else char_text.lower()\n",
    "                    if compare_text == search[j]:\n",
    "                        matched_chars.append(chars[i + j])\n",
    "                    else:\n",
    "                        match = False\n",
    "                        break\n",
    "                else:\n",
    "                    match = False\n",
    "                    break\n",
    "\n",
    "            if match and matched_chars:\n",
    "                x0 = min(char['x0'] for char in matched_chars)\n",
    "                y0 = min(char['top'] for char in matched_chars)\n",
    "                x1 = max(char['x1'] for char in matched_chars)\n",
    "                y1 = max(char['bottom'] for char in matched_chars)\n",
    "\n",
    "                positions.append(\n",
    "                    {\n",
    "                        'page': page_num,\n",
    "                        'method': 'chars',\n",
    "                        'text': search_text,\n",
    "                        'bbox': {'x0': x0, 'y0': y0, 'x1': x1, 'y1': y1},\n",
    "                        'center': {'x': (x0 + x1) / 2, 'y': (y0 + y1) / 2},\n",
    "                        'page_size': {'width': page.width, 'height': page.height},\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return positions\n",
    "\n",
    "    def _find_with_words(self, page, page_num, search_text, case_sensitive):\n",
    "        \"\"\"使用单词对象查找\"\"\"\n",
    "        positions = []\n",
    "        words = page.extract_words()\n",
    "        search = search_text if case_sensitive else search_text.lower()\n",
    "\n",
    "        for word in words:\n",
    "            word_text = word['text'] if case_sensitive else word['text'].lower()\n",
    "            if search in word_text:\n",
    "                positions.append(\n",
    "                    {\n",
    "                        'page': page_num,\n",
    "                        'method': 'words',\n",
    "                        'text': search_text,\n",
    "                        'matched_text': word['text'],\n",
    "                        'bbox': {\n",
    "                            'x0': word['x0'],\n",
    "                            'y0': word['top'],\n",
    "                            'x1': word['x1'],\n",
    "                            'y1': word['bottom'],\n",
    "                        },\n",
    "                        'center': {\n",
    "                            'x': (word['x0'] + word['x1']) / 2,\n",
    "                            'y': (word['top'] + word['bottom']) / 2,\n",
    "                        },\n",
    "                        'page_size': {'width': page.width, 'height': page.height},\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return positions\n",
    "\n",
    "    def _find_with_text(self, page, page_num, search_text, case_sensitive):\n",
    "        \"\"\"使用页面文本查找（近似位置）\"\"\"\n",
    "        positions = []\n",
    "        text = page.extract_text()\n",
    "        search = search_text if case_sensitive else search_text.lower()\n",
    "        page_text = text if case_sensitive else text.lower()\n",
    "\n",
    "        if search in page_text:\n",
    "            # 这只能提供页面级别的信息，无法提供精确坐标\n",
    "            positions.append(\n",
    "                {\n",
    "                    'page': page_num,\n",
    "                    'method': 'text',\n",
    "                    'text': search_text,\n",
    "                    'approximate': True,\n",
    "                    'page_size': {'width': page.width, 'height': page.height},\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return positions\n",
    "\n",
    "    def save_positions(self, positions, output_path):\n",
    "        \"\"\"保存位置信息到JSON文件\"\"\"\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(positions, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"关闭PDF文件\"\"\"\n",
    "        self.pdf.close()\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "locator = PDFTextLocator(\n",
    "    r'D:\\user\\PythonProject\\AI_env2\\uploads\\1_tender_招标文件正文.pdf'\n",
    ")\n",
    "\n",
    "# 使用不同方法查找文本\n",
    "positions_chars = locator.find_text('评标办法', method='chars')\n",
    "print(positions_chars)\n",
    "# positions_words = locator.find_text('投标保证金', method='words')\n",
    "# print(positions_words)\n",
    "# 保存结果\n",
    "# locator.save_positions(positions_chars, '招标人_positions.json')\n",
    "# locator.save_positions(positions_words, '投标保证金_positions.json')\n",
    "\n",
    "# 关闭文件\n",
    "locator.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe53fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "class AdvancedTableMerger:\n",
    "    def __init__(self, pdf_path: str):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.pdf = pdfplumber.open(pdf_path)\n",
    "\n",
    "    def analyze_table_content(self, table: List[List]) -> Dict:\n",
    "        \"\"\"\n",
    "        分析表格内容特征\n",
    "        \"\"\"\n",
    "        if not table:\n",
    "            return {}\n",
    "\n",
    "        analysis = {\n",
    "            'row_count': len(table),\n",
    "            'col_count': max(len(row) for row in table) if table else 0,\n",
    "            'cell_count': sum(len(row) for row in table),\n",
    "            'non_empty_cells': sum(sum(1 for cell in row if cell) for row in table),\n",
    "            'content_types': self._analyze_content_types(table),\n",
    "        }\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def _analyze_content_types(self, table: List[List]) -> Dict:\n",
    "        \"\"\"\n",
    "        分析表格内容类型\n",
    "        \"\"\"\n",
    "        types = {'numeric': 0, 'text': 0, 'mixed': 0, 'empty': 0}\n",
    "\n",
    "        for row in table:\n",
    "            for cell in row:\n",
    "                if not cell:\n",
    "                    types['empty'] += 1\n",
    "                elif self._is_numeric(str(cell)):\n",
    "                    types['numeric'] += 1\n",
    "                elif self._is_text(str(cell)):\n",
    "                    types['text'] += 1\n",
    "                else:\n",
    "                    types['mixed'] += 1\n",
    "\n",
    "        return types\n",
    "\n",
    "    def _is_numeric(self, text: str) -> bool:\n",
    "        \"\"\"判断是否为数字\"\"\"\n",
    "        try:\n",
    "            float(text.replace(',', '').replace(' ', ''))\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    def _is_text(self, text: str) -> bool:\n",
    "        \"\"\"判断是否为纯文本\"\"\"\n",
    "        return bool(text and not self._is_numeric(text) and text.strip())\n",
    "\n",
    "    def calculate_table_similarity(self, table1: Dict, table2: Dict) -> float:\n",
    "        \"\"\"\n",
    "        计算两个表格的相似度\n",
    "        \"\"\"\n",
    "        # 1. 列数匹配度\n",
    "        col_match = 1.0 if table1['cols'] == table2['cols'] else 0.0\n",
    "\n",
    "        # 2. 内容类型相似度\n",
    "        types1 = table1.get('content_analysis', {}).get('content_types', {})\n",
    "        types2 = table2.get('content_analysis', {}).get('content_types', {})\n",
    "\n",
    "        type_similarity = self._calculate_type_similarity(types1, types2)\n",
    "\n",
    "        # 3. 表头相似度\n",
    "        header_similarity = self.calculate_header_similarity(\n",
    "            table1['headers'], table2['headers']\n",
    "        )\n",
    "\n",
    "        # 综合相似度\n",
    "        similarity = 0.3 * col_match + 0.4 * type_similarity + 0.3 * header_similarity\n",
    "        return similarity\n",
    "\n",
    "    def _calculate_type_similarity(self, types1: Dict, types2: Dict) -> float:\n",
    "        \"\"\"计算内容类型相似度\"\"\"\n",
    "        if not types1 or not types2:\n",
    "            return 0.0\n",
    "\n",
    "        total1 = sum(types1.values())\n",
    "        total2 = sum(types2.values())\n",
    "\n",
    "        if total1 == 0 or total2 == 0:\n",
    "            return 0.0\n",
    "\n",
    "        # 计算各类别的比例差异\n",
    "        similarity = 0.0\n",
    "        for key in set(types1.keys()) | set(types2.keys()):\n",
    "            ratio1 = types1.get(key, 0) / total1\n",
    "            ratio2 = types2.get(key, 0) / total2\n",
    "            similarity += 1.0 - abs(ratio1 - ratio2)\n",
    "\n",
    "        return similarity / len(set(types1.keys()) | set(types2.keys()))\n",
    "\n",
    "    def extract_enhanced_tables(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        提取增强版表格信息\n",
    "        \"\"\"\n",
    "        tables_info = []\n",
    "\n",
    "        for page_num, page in enumerate(self.pdf.pages, 1):\n",
    "            tables = page.extract_tables()\n",
    "\n",
    "            for table_index, table in enumerate(tables):\n",
    "                if table and len(table) > 0:\n",
    "                    # 基本信息\n",
    "                    rows = len(table)\n",
    "                    cols = max(len(row) for row in table) if table else 0\n",
    "                    headers = table[0] if table else []\n",
    "\n",
    "                    # 内容分析\n",
    "                    content_analysis = self.analyze_table_content(table)\n",
    "\n",
    "                    tables_info.append(\n",
    "                        {\n",
    "                            'page': page_num,\n",
    "                            'table_index': table_index,\n",
    "                            'rows': rows,\n",
    "                            'cols': cols,\n",
    "                            'headers': headers,\n",
    "                            'content_length': sum(\n",
    "                                len(str(cell)) for row in table for cell in row if cell\n",
    "                            ),\n",
    "                            'data': table,\n",
    "                            'content_analysis': content_analysis,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        return tables_info\n",
    "\n",
    "    def smart_merge_tables(self, tables_info: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        智能合并表格\n",
    "        \"\"\"\n",
    "        if not tables_info:\n",
    "            return []\n",
    "\n",
    "        merged_tables = []\n",
    "        i = 0\n",
    "\n",
    "        while i < len(tables_info):\n",
    "            current_table = tables_info[i]\n",
    "            merged_data = [current_table['data']]\n",
    "            merged_pages = [current_table['page']]\n",
    "\n",
    "            # 向后查找可能的连续表格\n",
    "            j = i + 1\n",
    "            while j < len(tables_info):\n",
    "                next_table = tables_info[j]\n",
    "\n",
    "                # 检查是否应该合并\n",
    "                should_merge = self._should_merge_tables(\n",
    "                    current_table, next_table, merged_pages\n",
    "                )\n",
    "\n",
    "                if should_merge:\n",
    "                    # 合并数据（去掉可能的重复表头）\n",
    "                    next_data = next_table['data']\n",
    "                    if len(next_data) > 1 and self._are_headers_similar(\n",
    "                        current_table['headers'], next_table['headers']\n",
    "                    ):\n",
    "                        # 如果表头相似，跳过下一张表的表头行\n",
    "                        merged_data.append(next_data[1:])\n",
    "                    else:\n",
    "                        merged_data.append(next_data)\n",
    "\n",
    "                    merged_pages.append(next_table['page'])\n",
    "                    current_table = next_table\n",
    "                    j += 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            # 合并所有数据\n",
    "            final_data = []\n",
    "            for data_chunk in merged_data:\n",
    "                final_data.extend(data_chunk)\n",
    "\n",
    "            # 创建合并后的表格\n",
    "            merged_table = {\n",
    "                'start_page': tables_info[i]['page'],\n",
    "                'end_page': current_table['page'],\n",
    "                'pages': merged_pages,\n",
    "                'page_count': len(merged_pages),\n",
    "                'total_rows': len(final_data),\n",
    "                'cols': tables_info[i]['cols'],\n",
    "                'headers': tables_info[i]['headers'],\n",
    "                'data': final_data,\n",
    "                'content_analysis': tables_info[i].get('content_analysis', {}),\n",
    "            }\n",
    "\n",
    "            merged_tables.append(merged_table)\n",
    "            i = j\n",
    "\n",
    "        return merged_tables\n",
    "\n",
    "    def _should_merge_tables(\n",
    "        self, table1: Dict, table2: Dict, merged_pages: List[int]\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        判断是否应该合并两个表格\n",
    "        \"\"\"\n",
    "        # 1. 必须是连续页面\n",
    "        if table2['page'] != table1['page'] + 1:\n",
    "            return False\n",
    "\n",
    "        # 2. 已经合并过的页面不重复处理\n",
    "        if table2['page'] in merged_pages:\n",
    "            return False\n",
    "\n",
    "        # 3. 列数必须相同\n",
    "        if table1['cols'] != table2['cols']:\n",
    "            return False\n",
    "\n",
    "        # 4. 计算相似度\n",
    "        similarity = self.calculate_table_similarity(table1, table2)\n",
    "\n",
    "        # 5. 如果相似度足够高，则合并\n",
    "        return similarity > 0.6\n",
    "\n",
    "    def _are_headers_similar(self, headers1: List, headers2: List) -> bool:\n",
    "        \"\"\"\n",
    "        判断表头是否相似\n",
    "        \"\"\"\n",
    "        if not headers1 or not headers2:\n",
    "            return False\n",
    "\n",
    "        similarity = self.calculate_header_similarity(headers1, headers2)\n",
    "        return similarity > 0.7\n",
    "\n",
    "    def calculate_header_similarity(self, headers1: List, headers2: List) -> float:\n",
    "        \"\"\"\n",
    "        计算表头相似度\n",
    "        \"\"\"\n",
    "        if not headers1 or not headers2:\n",
    "            return 0.0\n",
    "\n",
    "        max_len = max(len(headers1), len(headers2))\n",
    "        matches = 0\n",
    "\n",
    "        for i in range(min(len(headers1), len(headers2))):\n",
    "            h1 = str(headers1[i]).strip() if headers1[i] else ''\n",
    "            h2 = str(headers2[i]).strip() if headers2[i] else ''\n",
    "\n",
    "            if h1 and h2:\n",
    "                # 字符串包含关系或完全相等\n",
    "                if h1 == h2 or h1 in h2 or h2 in h1:\n",
    "                    matches += 1\n",
    "            elif h1 == h2:  # 都为空\n",
    "                matches += 1\n",
    "\n",
    "        return matches / max_len if max_len > 0 else 0.0\n",
    "\n",
    "    def process_tables(self, output_path: str = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        主处理方法\n",
    "        \"\"\"\n",
    "        # 1. 提取增强版表格信息\n",
    "        tables_info = self.extract_enhanced_tables()\n",
    "\n",
    "        # 2. 智能合并表格\n",
    "        merged_tables = self.smart_merge_tables(tables_info)\n",
    "\n",
    "        # 3. 转换为结构化格式\n",
    "        structured_tables = []\n",
    "        for table_info in merged_tables:\n",
    "            headers = table_info['headers']\n",
    "            data = table_info['data']\n",
    "\n",
    "            # 转换为键值对格式\n",
    "            rows = []\n",
    "            if len(data) > 1:\n",
    "                data_rows = data[1:]  # 跳过表头行\n",
    "                for row in data_rows:\n",
    "                    if len(row) == len(headers):\n",
    "                        row_dict = dict(zip(headers, row))\n",
    "                        rows.append(row_dict)\n",
    "\n",
    "            structured_table = {\n",
    "                'metadata': {\n",
    "                    'start_page': table_info['start_page'],\n",
    "                    'end_page': table_info['end_page'],\n",
    "                    'pages': table_info['pages'],\n",
    "                    'page_count': table_info['page_count'],\n",
    "                    'total_rows': table_info['total_rows'],\n",
    "                    'cols': table_info['cols'],\n",
    "                },\n",
    "                'headers': headers,\n",
    "                'rows': rows,\n",
    "                'content_analysis': table_info['content_analysis'],\n",
    "            }\n",
    "\n",
    "            structured_tables.append(structured_table)\n",
    "\n",
    "        # 4. 保存结果\n",
    "        if output_path:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(structured_tables, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        return structured_tables\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"关闭PDF文件\"\"\"\n",
    "        self.pdf.close()\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "def advanced_main():\n",
    "    merger = AdvancedTableMerger(\n",
    "        r'D:\\user\\PythonProject\\AI_env2\\uploads\\1_tender_招标文件正文.pdf'\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        result = merger.process_tables('advanced_merged_tables.json')\n",
    "\n",
    "        print(f'智能识别到 {len(result)} 个跨页表格:')\n",
    "        for i, table in enumerate(result, 1):\n",
    "            meta = table['metadata']\n",
    "            print(f'表格 {i}:')\n",
    "            print(\n",
    "                f'  - 跨页范围: 第{meta[\"start_page\"]}页到第{meta[\"end_page\"]}页 (共{meta[\"page_count\"]}页)'\n",
    "            )\n",
    "            print(f'  - 页面列表: {meta[\"pages\"]}')\n",
    "            print(f'  - 总行数: {meta[\"total_rows\"]}')\n",
    "            print(f'  - 列数: {meta[\"cols\"]}')\n",
    "            print(\n",
    "                f'  - 表头: {[h[:20] for h in table[\"headers\"][:5]]}'\n",
    "            )  # 显示前5个表头的前20个字符\n",
    "            print(f'  - 数据行数: {len(table[\"rows\"])}')\n",
    "            print()\n",
    "\n",
    "    finally:\n",
    "        merger.close()\n",
    "\n",
    "\n",
    "# 运行高级示例\n",
    "# advanced_main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
